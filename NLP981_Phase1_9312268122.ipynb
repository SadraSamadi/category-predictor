{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NLP981_Phase1_9312268122.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.5"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"0Tduc6QDQz1H"},"source":["# NLP981 Final Project - Phase #1"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"P0F0hIUs7oCS"},"source":["*   Instructor: Javad PourMostafa\n","*   Teaching Assistant: Parsa Abbasi\n","*   University of Guilan, 1st semester of 2019\n","*   GitHub repository : *https://github.com/JoyeBright/NLP*"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"rVC1mwiaOZMI","colab":{}},"source":["# !pip install nltk"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"aUO9K1mKO2EB","colab":{}},"source":["# import nltk\n","# nltk.download('punkt')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"hJiCXKiUNsUQ","colab":{}},"source":["# sentence = \"Hello, world!\"\n","# print(nltk.word_tokenize(sentence))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"_aE9xQhhQ6XS"},"source":["It's the first phase of your final project for the *NLP981* course. The main idea behind this phase is to portray the develope side of *NLP*.\n","\n","You must code inside of this python notebook. I highly recommend you to use the *Google Colab* environment. \n","\n","If you have any questions, feel free to ask.\n","You can use [*Quera*](https://quera.ir/course/4385/) platform for your general questions.\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"EgS3aCY358dV"},"source":["## Introduction"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"6FG4cndz6DDq"},"source":["A category predictor is going to build at this phase of the project.\n","\n","The predictor gets a text as input and predicts a category for that.\n","\n","For this purpose, you need to :\n","\n","1.   Load the dataset\n","2.   Preprocess the text data\n","3.   Implement a word representation method to represent each text as a numeric vector\n","4.   Implement a classification model and train that using the training set\n","5.   Predict a category for each of validation data using implemented model\n","6.   Measure your work using confusion matrix and some common metrics\n","\n","**Important Note:** You can use any library you want in sections 1 and 2. But everything in section 3-6 need to be coded purely.\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"0acil66KQ8A_"},"source":["## 1) Dataset"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"haH1kfj3Q9tf"},"source":["The dataset you will use in this phase is called *Divar* that released by the *CafeBazaar* research team.\n","\n","It contains more than 900,000 posts of the *Divar* ads platform. We split this dataset into training, validation, and testing sets.\n","\n","The testing set is not accessible for you, and we use them to evaluate your work on the presentation day.\n","\n","You can download the dataset files (training and validation sets) directly from the following link :\n","\n","> *https://drive.google.com/open?id=1oj-fqpymjDr8QsOK-zQliiqXbVqakrFo*\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"UdIRg1UBSOEi"},"source":["### 1.1) Import"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"gwCPJjaSQukm","colab":{}},"source":["%%time\n","\n","import pandas as pd\n","\n","# Loading data from Google Drive.\n","# from google.colab import drive\n","# drive.mount('/gdrive')\n","# train_set = pd.read_csv('/gdrive/My Drive/trainset.csv')\n","# valid_set = pd.read_csv('/gdrive/My Drive/validationset.csv')\n","\n","# Loading data from local.\n","train_set = pd.read_csv('trainset.csv')\n","valid_set = pd.read_csv('validationset.csv')\n","\n","# Combining the title and description columns together.\n","train_set['doc'] = train_set.title + ' ' + train_set.desc\n","valid_set['doc'] = valid_set.title + ' ' + valid_set.desc"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"MWHwVPkfS-OX"},"source":["### 1.2) Analyzing"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"KlIKCffqU4AM"},"source":["Display the top 10 rows of the train set."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Gw0EuzrFVIat","colab":{}},"source":["train_set.head(10)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"iqfkJST6VMXt"},"source":["How many data (rows) stored in the training and validation sets?"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Jkiaz92zVVpy","colab":{}},"source":["train_len = len(train_set)\n","valid_len = len(valid_set)\n","print('rows in the training set:', train_len)\n","print('rows in the validation set:', valid_len)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"9aw0x_s3_b4U"},"source":["How many posts are in each category (First level categories)? (Based on training set)"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"31PFgy46_ntw","colab":{}},"source":["train_set.cat1.hist(figsize=(12, 6))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"KUejNXljlZD1"},"source":["## 2) Preprocessing"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"-j1EilpbAi-W"},"source":["There are two kinds of text data in the dataset: *Title* and *Description*.\n","You can use one or both of them as text inputs of your classification model. Choose a composition that gives you a higher measuring score."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"cLT50bemjw3K"},"source":["You need to apply some preprocessing procedures on your text data first. We want at least **4** preprocessing step from you. It can be removing stop words, removing punctation, removing or replacing digits, stemming, lemmatizing, normalization, and so on.\n","\n","You can use the [*Stopwords Guilan NLP*](https://github.com/JoyeBright/stopwords_guilannlp) library to access a collection of Persian stop words."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"tuEjYQnNZqlC","colab":{}},"source":["!pip install hazm stopwords_guilannlp"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"xPQYAeEnlrcc","colab":{}},"source":["# This importation is required for the 'hazm' library.\n","from __future__ import unicode_literals\n","\n","import re\n","\n","from hazm import Normalizer, Stemmer, Lemmatizer, word_tokenize\n","from stopwords_guilannlp import stopwords_output\n","\n","\n","class PreProcessor:\n","    \"\"\"\n","    Document pre processes.\n","    \"\"\"\n","\n","    def __init__(self):\n","        self._normalizer = Normalizer()\n","        self._stemmer = Stemmer()\n","        self._lemmatizer = Lemmatizer()\n","        # The 'set' output type is faster than others.\n","        self._stopwords = stopwords_output('persian', 'set')\n","\n","    def pre_process(self, doc):\n","        \"\"\"\n","        Extract clear words from the document.\n","        \"\"\"\n","        scrubbed = self._scrub(doc)\n","        normalized = self._normalizer.normalize(scrubbed)\n","        tokens = word_tokenize(normalized)\n","        words = []\n","        for token in tokens:\n","            # Check if token is not a stop-word.\n","            if token not in self._stopwords:\n","                stemmed = self._stemmer.stem(token)\n","                # Sometimes stem method returns an empty string,\n","                # so we should check that and pass the original token to lemmatize.\n","                lemmatized = self._lemmatizer.lemmatize(stemmed or token)\n","                words.append(lemmatized)\n","        return words\n","\n","    def _scrub(self, text):\n","        \"\"\"\n","        Remove unexpected characters from the text.\n","        \"\"\"\n","        # Remove persian symbols.\n","        text = re.sub(r'[۰۱۲۳۴۵۶۷۸۹…«»،؟]+', ' ', text)\n","        # Remove ascii symbols.\n","        text = re.sub(r'[-!@#$%^&*()_+|~=`{}\\[\\]:\";\\'<>?,.\\\\/\\d]+', ' ', text)\n","        # Remove extra spaces.\n","        text = re.sub(r'\\s+', ' ', text)\n","        # Trim the text.\n","        return text.strip()\n","\n","\n","pre_processor = PreProcessor()\n","\n","\n","def preprocessing(text):\n","    return pre_processor.pre_process(text)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"izN13hPulonI"},"source":["## 3) Word Representation"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"UeC-rd4DMSUb"},"source":["As you know, classification models can't deal with strings directly, and you have to represent your texts in a numerical form."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"S1rGMFJmqKwm"},"source":["### 3.1) Tf-idf"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"VfPOmAF6qOF0"},"source":["You have to implement the tf-idf vectorization method from scratch in this step. \n","\n","Furthermore, a function must be implemented that gives a text input and return a tf-idf vectorized representation."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"nTZCMdqft4Ic"},"source":["$$\\text{tf-idf}(t, d) = \\text{tf}(t, d) \\times \\text{idf}(t)$$\n","\n","*tf* (term-frequency) is the count of occurrences of the word `t` in specific text `d`.\n","\n","*idf* (inverse document-frequency) is term that is inversely proportional to the number of texts with the given word. It can be calculated this way:\n","$$\\text{idf}(t) = \\text{log}\\frac{1 + n_d}{1 + n_{d(t)}} + 1$$\n","where $n_d$ is the whole number of texts and $n_{d(t)}$ is the number of texts with the word `t`."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ZVwNZTGLqm20","colab":{}},"source":["from collections import defaultdict\n","\n","import numpy as np\n","import scipy.sparse as sp\n","\n","\n","class Vectorizer:\n","    \"\"\"\n","    TF-IDF vectorizer.\n","    \"\"\"\n","\n","    def __init__(self, pre_process):\n","        \"\"\"\n","        Parameters:\n","            pre_process: callable\n","                A function that returns cleared tokens of a document.\n","        \"\"\"\n","        self._pre_process = pre_process\n","        self._vocabulary = None\n","        self._idf = None\n","\n","    def fit(self, docs):\n","        \"\"\"\n","        Learn vocabulary and create an IDF matrix for the documents.\n","\n","        Returns:\n","            The IDF matrix.\n","        \"\"\"\n","        tf = self._tf(docs)\n","        samples = len(docs)\n","        features = len(self._vocabulary)\n","        # Count number of occurrences of each column index (word).\n","        ndt = np.bincount(tf.indices, minlength=features)\n","        # Calculate the IDF vector.\n","        idf = np.log((1 + samples) / (1 + ndt)) + 1\n","        # For creating a TF-IDF matrix we should multiply each row of the TF matrix\n","        # by the IDF vector, so to simplify the calculation it's better to convert\n","        # the IDF vector to a diagonal CSR matrix.\n","        self._idf = sp.diags(idf, format='csr', shape=(features, features))\n","        return tf\n","\n","    def fit_transform(self, docs):\n","        \"\"\"\n","        Learn vocabulary and create a TF-IDF matrix.\n","        \"\"\"\n","        tf = self.fit(docs)\n","        return self._transform(tf)\n","\n","    def transform(self, docs):\n","        \"\"\"\n","        Create a TF-IDF matrix for the documents by multiplying,\n","        the TF and the IDF matrices.\n","        \"\"\"\n","        tf = self._tf(docs)\n","        return self._transform(tf)\n","\n","    def _transform(self, tf):\n","        return tf @ self._idf\n","\n","    def _tf(self, docs):\n","        \"\"\"\n","        Create a Term-Frequency matrix for the documents.\n","        \"\"\"\n","        # If it's the first time (fitting time), we create a dictionary\n","        # that uses words as a key and generates an auto-increment index\n","        # for them as a value.\n","        vocabulary = self._vocabulary or defaultdict(lambda: len(vocabulary))\n","        data = []  # List of frequencies.\n","        indices = []  # List of column indices for each frequency.\n","        indptr = [0]  # Index pointer, indicates how many elements are in each row.\n","        for doc in docs:\n","            # We use a default dictionary to count each word in the current document.\n","            word_count = defaultdict(int)\n","            # Extract clear words from the document.\n","            for word in self._pre_process(doc):\n","                try:\n","                    index = vocabulary[word]\n","                    word_count[index] += 1\n","                except KeyError:\n","                    # If it's not fitting time and the word doesn't exist in the vocabulary,\n","                    # ignore it.\n","                    continue\n","            indices.extend(word_count.keys())\n","            data.extend(word_count.values())\n","            indptr.append(len(indices))\n","        if not self._vocabulary:\n","            self._vocabulary = dict(vocabulary)\n","        # Since the TF matrix has a lot of zeros, we should use a sparse matrix.\n","        # A Compressed Sparse Row (CSR) matrix is more efficient for arithmetic operations.\n","        samples, features = len(docs), len(vocabulary)\n","        return sp.csr_matrix((data, indices, indptr), shape=(samples, features))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"8yyUQ-_urWW5","colab":{}},"source":["%%time\n","\n","# Estimated time: 2min 20s\n","\n","vectorizer = Vectorizer(preprocessing)\n","\n","\n","def tf_idf(text):\n","    return vectorizer.transform(text)\n","\n","\n","x_train = vectorizer.fit_transform(train_set.doc)\n","x_valid = tf_idf(valid_set.doc)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"tSDMXv46CWwm"},"source":["## 4) Classification"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"mRpciSZZ-mb9"},"source":["![alt text](https://cdn.lynda.com/course/578082/578082-637075371482276339-16x9.jpg)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"tk1EJq7B4JwX"},"source":["### 4.1) Logistic Regression"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"MoiyVNNz-q5k"},"source":["The Logistic Regression classifier must be implemented from scratch here.\n","\n","You can fit the training data into the classifier after implementing linear regression."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"5q-s73zI-xOq","colab":{}},"source":["import numpy as np\n","import scipy.sparse as sp\n","\n","\n","class Classifier:\n","    \"\"\"\n","    Logistic Regression classifier.\n","    \"\"\"\n","\n","    def __init__(self, alpha=0.01, epochs=100):\n","        \"\"\"\n","        Parameters:\n","            alpha: float\n","                Learning rate.\n","            epochs: int\n","                Maximum number of iterations.\n","        \"\"\"\n","        self._alpha = alpha\n","        self._epochs = epochs\n","        self._classes = None\n","        self._thetas = None\n","\n","    def fit(self, x, y):\n","        \"\"\"\n","        Fit the model according to the given training data.\n","        \"\"\"\n","        x = self._add_bias(x)\n","        samples, features = x.shape\n","        # Find unique classes in the targets.\n","        self._classes = np.unique(y)\n","        self._thetas = []\n","        # For a multi-class model, we must fit the wights for each class separately.\n","        for clazz in self._classes:\n","            # Create a binary vector of targets, by one-vs-rest strategy,\n","            # where the current class is one and others are zero.\n","            bin_y = np.where(clazz == y, 1, 0)\n","            # Create a new weights vector for the class and initialize it with zeros.\n","            theta = np.zeros(features)\n","            # Repeat this for the given iterations number.\n","            for _ in range(self._epochs):\n","                # Evaluate the output probability.\n","                pred = self._probability(x, theta)\n","                # Calculate gradient ascent.\n","                grad = (bin_y - pred) @ x / samples\n","                # Update the weights by gradient ascent.\n","                theta += self._alpha * grad\n","            self._thetas.append(theta)\n","        return\n","\n","    def predict(self, x):\n","        \"\"\"\n","        Predict the classification for each sample in the given list.\n","        \"\"\"\n","        x = self._add_bias(x)\n","        # For each class, calculate the confidence scores of the given samples.\n","        scores = [self._probability(x, theta) for theta in self._thetas]\n","        # Find the maximum score in each column and give the row index of it,\n","        # which indicates the corresponding class index.\n","        indices = np.argmax(scores, axis=0)\n","        return self._classes[indices]\n","\n","    def _add_bias(self, x):\n","        \"\"\"\n","        Add a bias vector with values of one, as a new column,\n","        to the start of the data matrix.\n","        \"\"\"\n","        samples = x.shape[0]\n","        bias = np.ones((samples, 1))\n","        if sp.isspmatrix(x):\n","            # If the original data is a sparse matrix, use scipy module instead.\n","            return sp.hstack([bias, x])\n","        return np.hstack([bias, x])\n","\n","    def _probability(self, x, theta):\n","        \"\"\"\n","        Calculate the confidence score for the given data by its weights.\n","        \"\"\"\n","        # Linear model equation.\n","        z = x @ theta\n","        # Apply the sigmoid function to the output.\n","        return 1 / (1 + np.exp(-z))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"k089_muZ6j-j"},"source":["## 5) Prediction"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Kc5dl49O3aST"},"source":["Now you can predict a category for each of the validation data using the implemented classifier."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"0bM9Shkx3x7Z","colab":{}},"source":["%%time\n","\n","# Estimated time: 10min 40s\n","\n","y_train = train_set.cat1\n","y_valid = valid_set.cat1\n","\n","classifier = Classifier(alpha=0.1, epochs=1000)\n","classifier.fit(x_train, y_train)\n","predicted = classifier.predict(x_valid)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"JsxqAwVvwOdD"},"source":["## 6) Evaluation"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"75Ar7r9pyRLq"},"source":["It's time to evaluate your model using predicted categories for validation data.\n","\n","You need to create a confusion matrix based on your prediction and the real labels. Then you can use this confusion matrix for calculation other measuring metrics. \n","\n","As this problem is a multi-class problem, the calculation formula is a little different from the general case. Read [this article](https://towardsdatascience.com/multi-class-metrics-made-simple-part-i-precision-and-recall-9250280bddc2) for more information."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"bp1BRF8gIozE","colab":{}},"source":["import numpy as np\n","import pandas as pd\n","\n","\n","class Metrics:\n","    \"\"\"\n","    Classification metrics based on the Confusion Matrix.\n","    \"\"\"\n","\n","    def __init__(self):\n","        self._cm = None\n","\n","    def confusion_matrix(self, y_true, y_pred):\n","        \"\"\"\n","        Create the Confusion Matrix.\n","        \"\"\"\n","        # Find unique classes.\n","        classes = np.unique(y_true)\n","        # Create a data-frame for the confusion matrix,\n","        # because it's easy to work with later.\n","        # Rows of the DF are representing the predicted classes.\n","        # Columns of the DF are representing the true classes.\n","        self._cm = pd.DataFrame(0, index=classes, columns=classes)\n","        for true, pred in zip(y_true, y_pred):\n","            self._cm[true][pred] += 1\n","        return self._cm\n","\n","    def classification_report(self):\n","        \"\"\"\n","        Show the main classification metrics.\n","        \"\"\"\n","        classes = self._cm.index\n","        return pd.DataFrame([{\n","            'precision': self.precision_score(clazz),\n","            'recall': self.recall_score(clazz),\n","            'f1-score': self.f1_score(clazz),\n","            'support': self._cm[clazz].sum()\n","        } for clazz in classes], index=classes)\n","\n","    def precision_score(self, clazz=None, average=True):\n","        \"\"\"\n","        Calculate the Precision Score from the confusion matrix.\n","\n","        Parameters:\n","            clazz: object\n","                If presented, the score will be calculated just for this class.\n","            average: bool\n","                Should return score for each class or calculate the macro average?\n","        \"\"\"\n","        if clazz:\n","            return self._portion(self._cm.loc[clazz])\n","        precisions = self._cm.apply(self._portion, axis=1)\n","        return precisions.mean() if average else precisions\n","\n","    def recall_score(self, clazz=None, average=True):\n","        \"\"\"\n","        Calculate the Recall Score from the confusion matrix.\n","\n","        Parameters:\n","            clazz: object\n","                If presented, the score will be calculated just for this class.\n","            average: bool\n","                Should return score for each class or calculate the macro average?\n","        \"\"\"\n","        if clazz:\n","            return self._portion(self._cm[clazz])\n","        recalls = self._cm.apply(self._portion, axis=0)\n","        return recalls.mean() if average else recalls\n","\n","    def f1_score(self, clazz=None, average=True):\n","        \"\"\"\n","        Calculate the F1 Score from the confusion matrix.\n","\n","        Parameters:\n","            clazz: object\n","                If presented, the score will be calculated just for this class.\n","            average: bool\n","                Should return score for each class or calculate the macro average?\n","        \"\"\"\n","        precision = self.precision_score(clazz, False)\n","        recall = self.recall_score(clazz, False)\n","        f1 = 2 * recall * precision / (recall + precision)\n","        return f1.mean() if average else f1\n","\n","    def accuracy_score(self):\n","        \"\"\"\n","        Calculate the Accuracy Score from the confusion matrix.\n","        \"\"\"\n","        cm = self._cm.values\n","        return cm.diagonal().sum() / cm.sum()\n","\n","    def _portion(self, vector):\n","        \"\"\"\n","        Calculate the portion of an element in the vector along its axis.\n","        \"\"\"\n","        return vector[vector.name] / vector.sum()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"WZ-19cA3wiqT"},"source":["### 6.1) Confusion matrix"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"KB5CCVKbw2Po","colab":{}},"source":["# Estimated time: 15s\n","\n","metrics = Metrics()\n","metrics.confusion_matrix(y_valid, predicted)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"DAAiuxrmJCtu","colab":{}},"source":["metrics.classification_report()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"SM6GgtHNwl1a"},"source":["### 6.2) Accuracy"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"9hL4wy510lEw"},"source":["$$\\text{Accuracy} = \\frac{TP + TN}{TP + FP + FN + TN}$$"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"eXPscPgkw4_S","colab":{}},"source":["accuracy = metrics.accuracy_score()\n","print(accuracy)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"nTFur0U5wsmO"},"source":["### 6.3) Precision"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"frrTkQWG09fd"},"source":["$$\\text{Precision} = \\frac{TP}{TP + FP}$$"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"q1SGaY9qw7Z4","colab":{}},"source":["precision = metrics.precision_score()\n","print(precision)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"lQcpDrm1wuhU"},"source":["### 6.4) Recall"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"QBb1eop61JlG"},"source":["$$\\text{Recall} = \\frac{TP}{TP + FN}$$"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"w8bLKc6Ew_iR","colab":{}},"source":["recall = metrics.recall_score()\n","print(recall)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"VWx42Dr5wx4m"},"source":["### 6.5) F1 score"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"hdVnZfm21SCM"},"source":["$$\\text{F1 score} = 2\\times \\frac{(Recall \\times  Precision)}{Recall + Precision}$$ "]},{"cell_type":"code","metadata":{"colab_type":"code","id":"4FkCog8ExBXA","colab":{}},"source":["f1_score = metrics.f1_score()\n","print(f1_score)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"TZP1GHbtB2T7"},"source":["## 7) K-Fold Cross Validation *(Optional)*"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ayL96oOoB-aq"},"source":["Evaluate your model based on the K-Fold Cross Validation approach. This step is optional and has a few extra points."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"-Zxs_90TC-3b","colab":{}},"source":["%%time\n","\n","# Estimated time: 12min 40s\n","\n","import numpy as np\n","import scipy.sparse as sp\n","\n","x = sp.vstack([x_train, x_valid])\n","y = y_train.append(y_valid)\n","\n","\n","def split(folds):\n","    samples = x.shape[0]\n","    fold = samples // folds\n","    indices = np.arange(samples)\n","    np.random.shuffle(indices)\n","    for i in range(folds):\n","        start = fold * i\n","        end = fold * (i + 1)\n","        valid = indices[start:end]\n","        left = indices[:start]\n","        right = indices[end:]\n","        train = np.concatenate([left, right])\n","        yield x[train], x[valid], y.iloc[train], y.iloc[valid]\n","\n","\n","def cross_validation():\n","    scores = []\n","    classifier = Classifier(alpha=0.1, epochs=200)\n","    for x_t, x_v, y_t, y_v in split(5):\n","        classifier.fit(x_t, y_t)\n","        preds = classifier.predict(x_v)\n","        metrics.confusion_matrix(y_v, preds)\n","        score = metrics.accuracy_score()\n","        scores.append(score)\n","    return np.average(scores)\n","\n","\n","cv_accuracy = cross_validation()\n","print('K-Fold Cross Validation Accuracy:', cv_accuracy)"],"execution_count":0,"outputs":[]}]}